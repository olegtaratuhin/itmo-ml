{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import abc\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(30)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1\n",
    "\n",
    "**Description**\n",
    "Apply the logistic regression method using the functions in the classwork notebook to predict biological response of a molecule. Data (bioresponse) from Kaggle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Helper functions from classwork**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0.\n",
    "\n",
    "    return w, b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Forward and backward propagation**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def propagate(w, b, X, y):\n",
    "\n",
    "    m = X.shape[1]\n",
    "    #print('number of objects = ',len(X))\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    a = sigmoid(np.dot(w.T, X) + b)\n",
    "    cost = -(1. / m) * np.sum(y * np.log(a) + (1 - y) * np.log(1 - a), axis = 1)\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    dw = (1. / m) * np.dot(X, (a - y).T)\n",
    "    db = (1. / m) * np.sum(a - y, axis = 1)\n",
    "\n",
    "    grads = {\"dw\": dw, \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w, b, X, y = np.array([[1.],[-1.]]), 4., np.array([[1.,5.,-1.],[10.,0.,-3.2]]), np.array([[0,1,1]])\n",
    "grads, cost = propagate(w, b, X, y)\n",
    "print(\"dw = \" + str(grads[\"dw\"]))\n",
    "print(\"db = \" + str(grads[\"db\"]))\n",
    "print(\"cost = \" + str(cost))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Model optimization**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def optimize(w, b, X, y, num_iterations, learning_rate, print_cost = False):\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "                \n",
    "        # Cost and gradient calculation \n",
    "        grads, cost = propagate(w, b, X, y)\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (f\"Cost after iteration {i}: {cost}\")\n",
    "\n",
    "    params = { \"w\": w, \"b\": b }\n",
    "    grads = { \"dw\": dw, \"db\": db }\n",
    "    \n",
    "    return params, grads, costs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params, grads, costs = optimize(w, b, X, y, num_iterations= 1000, learning_rate = 0.005, print_cost = True)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Predictions**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    m = X.shape[1]\n",
    "    y_pred = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "\n",
    "    # Compute vector \"A\" predicting the probabilities\n",
    "    a = sigmoid(np.dot(w.T, X) + b)\n",
    "\n",
    "    for i in range(a.shape[1]):\n",
    "\n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        if a[0,i] <= 0.5:\n",
    "            y_pred[0][i] = 0\n",
    "        else:\n",
    "            y_pred[0][i] = 1\n",
    "\n",
    "    return y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w = np.array([[0.1124579],[0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Wrap into model**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def model(X_train, y_train, X_test, y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "\n",
    "    # initialize parameters with zeros\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent\n",
    "    parameters, grads, costs = optimize(w, b, X_train, y_train, num_iterations, learning_rate, print_cost)\n",
    "\n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "\n",
    "    # Predict test/train set examples\n",
    "    y_pred_test = predict(w, b, X_test)\n",
    "    y_pred_train = predict(w, b, X_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_pred_train - y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_pred_test - y_test)) * 100))\n",
    "\n",
    "    d = {\n",
    "        \"costs\": costs,\n",
    "        \"y_prediction_test\": y_pred_test,\n",
    "        \"y_prediction_train\" : y_pred_train,\n",
    "        \"learning_rate\" : learning_rate,\n",
    "        \"num_iterations\": num_iterations,\n",
    "        \"w\" : w,\n",
    "        \"b\" : b,\n",
    "    }\n",
    "\n",
    "    return d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Load data**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bioresponse.csv\")\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y = df[\"Activity\"]\n",
    "X = df.drop(columns=[\"Activity\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=30, test_size=0.25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Running the following (using unmodified code from class work yields shape incompatibility)**\n",
    "\n",
    "In order to have this as part of part 1 of task 1 is it here, but to make notebook actually runnable I will comment it out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model(\n",
    "#     X_train, y_train.T,\n",
    "#     X_test, y_test.T,\n",
    "#     num_iterations = 2000,\n",
    "#     learning_rate = 0.001,\n",
    "#     print_cost = True\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use SGD and Adam for training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Adam and SGD implementation**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LearningRateScheduler(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def get(self, iteration: int = None, loss: int = None) -> float:\n",
    "        pass\n",
    "\n",
    "class ConstantLearningRate(LearningRateScheduler):\n",
    "    def __init__(self, lr: float = 1e-7):\n",
    "        self.lr = lr\n",
    "\n",
    "    def get(self, iteration: int = None, loss: int = None) -> float:\n",
    "        return self.lr\n",
    "\n",
    "class Optimizer(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def step(self, parameters, gradient):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_batch_size(self):\n",
    "        pass\n",
    "\n",
    "class SGD(Optimizer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 batch_size: int = 16,\n",
    "                 lr: LearningRateScheduler = ConstantLearningRate()):\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, parameters, gradient):\n",
    "        delta = self.lr.get() * gradient\n",
    "        return delta\n",
    "\n",
    "    def get_batch_size(self) -> int:\n",
    "        return self.batch_size\n",
    "\n",
    "class Adam(Optimizer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 batch_size: int = 16,\n",
    "                 beta1: float = 0.9,\n",
    "                 beta2: float = 0.999,\n",
    "                 eps: float = 1e-7,\n",
    "                 lr: LearningRateScheduler = ConstantLearningRate()):\n",
    "        self.batch_size: int = batch_size\n",
    "        self.beta1: float = beta1\n",
    "        self.beta2: float = beta2\n",
    "        self.eps: float = eps\n",
    "        self.lr: LearningRateScheduler = lr\n",
    "        self.cache: dict = {\n",
    "            \"t\": -1,\n",
    "            \"mean\": None,\n",
    "            \"var\": None,\n",
    "        }\n",
    "\n",
    "    def get_batch_size(self) -> int:\n",
    "        return self.batch_size\n",
    "\n",
    "    def step(self, parameters, gradient):\n",
    "        if self.cache[\"t\"] == -1:\n",
    "            self.cache = {\n",
    "                \"t\": 0,\n",
    "                \"mean\": np.zeros_like(parameters),\n",
    "                \"variance\": np.zeros_like(parameters)\n",
    "            }\n",
    "\n",
    "        t = self.cache[\"t\"] + 1\n",
    "        mean = self.cache[\"mean\"]\n",
    "        variance = self.cache[\"variance\"]\n",
    "\n",
    "        self.cache[\"t\"] = t\n",
    "        self.cache[\"mean\"] = self.beta1 * mean + (1 - self.beta1) * gradient\n",
    "        self.cache[\"variance\"] = self.beta2 * variance + (1 - self.beta2) * gradient ** 2\n",
    "\n",
    "        v_hat = self.cache[\"variance\"] / (1. - (self.beta2 ** t))\n",
    "        m_hat = self.cache[\"mean\"] / (1. - (self.beta1 ** t))\n",
    "\n",
    "        update = self.lr.get(iteration=t) * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "        return update"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Helpers to define the model**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def softmax(predictions):\n",
    "    single = (predictions.ndim == 1)\n",
    "\n",
    "    if single:\n",
    "        predictions = predictions.reshape(1, predictions.shape[0])\n",
    "\n",
    "    maximums = np.amax(predictions, axis=1).reshape(predictions.shape[0], 1)\n",
    "    predictions_ts = predictions - maximums\n",
    "\n",
    "    predictions_exp = np.exp(predictions_ts)\n",
    "    sums = np.sum(predictions_exp, axis=1).reshape(predictions_exp.shape[0], 1)\n",
    "    result = predictions_exp / sums\n",
    "\n",
    "    if single:\n",
    "        result = result.reshape(result.size)\n",
    "\n",
    "    return result   # S\n",
    "\n",
    "def cross_entropy_loss(probs, target_index):\n",
    "    single = (probs.ndim == 1)\n",
    "\n",
    "    if single:\n",
    "        probs = probs.reshape(1, probs.shape[0])\n",
    "        target_index = np.array([target_index])\n",
    "\n",
    "    rows = np.arange(target_index.shape[0])\n",
    "    cols = target_index\n",
    "\n",
    "    return np.mean(-np.log(probs[rows, cols]))\n",
    "\n",
    "\n",
    "def softmax_with_cross_entropy(predictions, target_index):\n",
    "    single = (predictions.ndim == 1)\n",
    "\n",
    "    if single:\n",
    "        predictions = predictions.reshape(1, predictions.shape[0])\n",
    "        target_index = np.array([target_index])\n",
    "\n",
    "    probs = softmax(predictions)\n",
    "    loss = cross_entropy_loss(probs, target_index)\n",
    "\n",
    "    indicator = np.zeros(probs.shape)\n",
    "    indicator[np.arange(probs.shape[0]), target_index] = 1\n",
    "    dprediction = (probs - indicator) / predictions.shape[0]\n",
    "\n",
    "    if single:\n",
    "        dprediction = dprediction.reshape(dprediction.size)\n",
    "\n",
    "    return loss, dprediction\n",
    "\n",
    "def linear_softmax(X, W, target_index):\n",
    "    predictions = np.dot(X, W)\n",
    "    loss, dprediction = softmax_with_cross_entropy(predictions, target_index)\n",
    "\n",
    "    dW = np.matmul(dprediction.T, X).T\n",
    "\n",
    "    return loss, dW"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LinearSoftmaxClassifier:\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def fit(self, X, y, epochs=1, verbose=10, lr=0.001, X_eval: np.array = None, y_eval: np.array = None):\n",
    "        num_train = X.shape[0]\n",
    "        num_features = X.shape[1]\n",
    "        num_classes = np.max(y) + 1\n",
    "        if self.W is None:\n",
    "            self.W = 0.001 * np.random.randn(num_features, num_classes)\n",
    "\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            loss, dW = linear_softmax(X, self.W, y)\n",
    "            self.W -= lr * dW\n",
    "\n",
    "            if epoch % verbose  == 0:\n",
    "                if X_eval is not None and y_eval is not None:\n",
    "                    y_pred = self.predict(X_eval)\n",
    "                    print(f\"Epoch: \\t{epoch},\\t loss: {loss}, \\t accuracy: {(y_pred == y_eval).mean()}\")\n",
    "                else:\n",
    "                    print(f\"Epoch: \\t{epoch},\\t loss: {loss}\")\n",
    "\n",
    "            loss_history.append(loss)\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        Z = np.dot(X, self.W)\n",
    "        S = softmax(Z)\n",
    "\n",
    "        return np.argmax(S, axis=1)\n",
    "    \n",
    "\n",
    "model = LinearSoftmaxClassifier()\n",
    "history = model.fit(X_train.values, y_train.values, epochs=300, lr=0.1, X_eval=X_test, y_eval=y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So we got 0.76 accuracy, which is surprising considering how simple our model is"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2\n",
    "\n",
    "**Description**\n",
    "\n",
    "Modify the code so that it works for SGD, (gradients are evaluated in batches)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LinearSoftmaxClassifierBatched:\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def fit(self, X, y, epochs=1, verbose=10, optim = SGD(), X_eval: np.array = None, y_eval: np.array = None):\n",
    "        num_train = X.shape[0]\n",
    "        num_features = X.shape[1]\n",
    "        num_classes = np.max(y) + 1\n",
    "        if self.W is None:\n",
    "            self.W = 0.001 * np.random.randn(num_features, num_classes)\n",
    "\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            shuffled_indices = np.arange(num_train)\n",
    "            np.random.shuffle(shuffled_indices)\n",
    "            batch_size = optim.get_batch_size()\n",
    "            sections = np.arange(batch_size, num_train, batch_size)\n",
    "            batches_indices = np.array_split(shuffled_indices, sections)\n",
    "\n",
    "            loss = np.nan\n",
    "            for batch_indices in batches_indices:\n",
    "                batch_X = X[batch_indices, :]\n",
    "                batch_y = y[batch_indices]\n",
    "\n",
    "                fn_loss, fn_dW = linear_softmax(batch_X, self.W, batch_y)\n",
    "\n",
    "                loss = fn_loss\n",
    "                dW = fn_dW\n",
    "\n",
    "                self.W -= optim.step(self.W, dW)\n",
    "            \n",
    "            if epoch % verbose  == 0:\n",
    "                if X_eval is not None and y_eval is not None:\n",
    "                    y_pred = self.predict(X_eval)\n",
    "                    print(f\"Epoch: \\t{epoch},\\t loss: {loss}, \\t accuracy: {(y_pred == y_eval).mean()}\")\n",
    "                else:\n",
    "                    print(f\"Epoch: \\t{epoch},\\t loss: {loss}\")\n",
    "\n",
    "            loss_history.append(loss)\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        Z = np.dot(X, self.W)\n",
    "        S = softmax(Z)\n",
    "\n",
    "        return np.argmax(S, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gd_training(lr):\n",
    "    model = LinearSoftmaxClassifier()\n",
    "    history = model.fit(X_train.values, y_train.values, epochs=200, lr=lr, X_eval=X_test, y_eval=y_test)\n",
    "    plt.plot(history);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**lr = 0.0001**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gd_training(lr=0.0001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The curve is smooth (not surprising), both loss and accuracy are not that good so far"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**lr = 0.001**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gd_training(lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Getting better"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**lr = 0.01**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gd_training(lr=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Keeps getting better"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**lr = 0.01**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gd_training(lr=0.075)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is as good as it gets here for GD, the other ones are not better"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**lr = 0.5**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gd_training(lr=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's a fun shape, seems like we are oscillating around the local minima, but the lr scheduler is constant, so we are kinda stuck"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SGD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sgd_training(lr):\n",
    "    model = LinearSoftmaxClassifierBatched()\n",
    "    sgd = SGD(lr=ConstantLearningRate(lr), batch_size=64)\n",
    "    history = model.fit(X_train.values, y_train.values, epochs=150, optim=sgd, X_eval=X_test, y_eval=y_test)\n",
    "    plt.plot(history);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**lr = 0.0001**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sgd_training(lr=0.0001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seems like this lr is too small, or we need to train more epochs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**lr = 0.0005**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sgd_training(lr=0.0005)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Getting better for the loss, but accuracy doesn't go up, perhaps now the model tunes predictions to be closer to extremes (0 or 1), which doesn't affect accuracy metric but is important for the loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**lr = 0.001**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sgd_training(lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Less stable than the previous one, it gets to lower loss, but with higher variance for the loss. However, accuracy is the best we got so far"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**lr = 0.005**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sgd_training(lr=0.005)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**lr = 0.1**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sgd_training(lr=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Actually it turns out that this lr works better then we previously picked as the best one, what happens if we increase it even more?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**lr = 0.6**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sgd_training(lr=0.6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is for sure too high"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adam"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def adam_training(lr):\n",
    "    model = LinearSoftmaxClassifierBatched()\n",
    "    adam = Adam(lr=ConstantLearningRate(lr), batch_size=64)\n",
    "    history = model.fit(X_train.values, y_train.values, epochs=200, optim=adam, X_eval=X_test, y_eval=y_test)\n",
    "    plt.plot(history);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**lr = 0.0001**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "adam_training(lr=0.0001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are getting the results better than with GD and SDG, and the accuracy and loss seems to be more stable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Bellow are some other lr with Adam**\n",
    "\n",
    "In general it seems that (not very surprising) in most cases Adam is better than SGD both in stability and target metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "adam_training(lr=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "adam_training(lr=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "adam_training(lr=0.0001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "adam_training(lr=0.0005)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}